{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet Recognition Model\n",
    "\n",
    "by FlÃ¡via Carvalhido"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "- input (2): path to the image folder, path to the csv with image paths and classes\n",
    "- output (1): trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd \n",
    "import shutil\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from PyTorch Tutorials : https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetTrain:\n",
    "    def __init__(self, img_folder_path=\"./train\", img_csv=\"./train.csv\"):\n",
    "        self.img_folder_path = img_folder_path\n",
    "        self.img_csv = img_csv\n",
    "\n",
    "        # TODO: have way to test if folder has already needed structure and test if code can make the folder structure from csv\n",
    "        # # Create directory with needed structure img_dir/class/img\n",
    "        # os.mkdir('./temp_train')\n",
    "        # df = pd.read_csv(self.img_csv)\n",
    "\n",
    "        # self.class_names = df[df.columns[1]].unique()\n",
    "\n",
    "        # for n in self.class_names:\n",
    "        #     os.mkdir('./temp_train/'+n)\n",
    "\n",
    "        # for _, row in df.iterrows():\n",
    "        #     shutil.move(row[0], os.path.join('./temp_train/',row[1]))\n",
    "\n",
    "\n",
    "        # Preprocess images (same for test)\n",
    "        data_transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [\n",
    "                                     0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "        }\n",
    "\n",
    "        image_datasets = {'train': datasets.ImageFolder(self.img_folder_path, data_transforms['train'])}\n",
    "\n",
    "        self.dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'], batch_size=4, shuffle=True, num_workers=4)}\n",
    "        self.dataset_sizes = {'train': len(image_datasets['train'])}\n",
    "        self.class_names = image_datasets['train'].classes #override\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    def train(self): \n",
    "\n",
    "        model_ft = models.resnet18(pretrained=True)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        # size of each output sample is set to the number of classes\n",
    "        model_ft.fc = nn.Linear(num_ftrs, len(self.class_names))\n",
    "\n",
    "        model_ft = model_ft.to(self.device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Observe that all parameters are being optimized\n",
    "        optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "        # Decay LR by a factor of 0.1 every 7 epochs\n",
    "        exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "        best_model_wts = copy.deepcopy(model_ft.state_dict())\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(25):\n",
    "            print(f'Epoch {epoch}/{25 - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase #FIXME: include validation??\n",
    "            for phase in ['train']:\n",
    "              \n",
    "                model_ft.train()  # Set model to training mode\n",
    "                \n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in self.dataloaders[phase]:\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer_ft.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model_ft(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer_ft.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    exp_lr_scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / self.dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / self.dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model_ft.state_dict())\n",
    "\n",
    "         \n",
    "        # load best model weights\n",
    "        model_ft.load_state_dict(best_model_wts)\n",
    "        return model_ft\n",
    "\n",
    "\n",
    "    def help(self):\n",
    "        print(\"Class ResNetTrain will train a ResNet recognition model with the provided arguments (in order): the training images folder path and the training images csv path. After initializing the class with the previously mentioned arguments, just call the method \\\"train\\\" and a trained ResNet model will be returned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.8377 Acc: 0.7086\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.5474 Acc: 0.8158\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.4136 Acc: 0.8654\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.3142 Acc: 0.8988\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.3138 Acc: 0.9039\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.2533 Acc: 0.9217\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.2130 Acc: 0.9365\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.1234 Acc: 0.9673\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.0895 Acc: 0.9792\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.0927 Acc: 0.9761\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.0773 Acc: 0.9798\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.0647 Acc: 0.9845\n",
      "Epoch 12/24\n",
      "----------\n",
      "train Loss: 0.0662 Acc: 0.9849\n",
      "Epoch 13/24\n",
      "----------\n",
      "train Loss: 0.0582 Acc: 0.9880\n",
      "Epoch 14/24\n",
      "----------\n",
      "train Loss: 0.0646 Acc: 0.9859\n",
      "Epoch 15/24\n",
      "----------\n",
      "train Loss: 0.0588 Acc: 0.9870\n",
      "Epoch 16/24\n",
      "----------\n",
      "train Loss: 0.0533 Acc: 0.9882\n",
      "Epoch 17/24\n",
      "----------\n",
      "train Loss: 0.0545 Acc: 0.9884\n",
      "Epoch 18/24\n",
      "----------\n",
      "train Loss: 0.0740 Acc: 0.9852\n",
      "Epoch 19/24\n",
      "----------\n",
      "train Loss: 0.0593 Acc: 0.9861\n",
      "Epoch 20/24\n",
      "----------\n",
      "train Loss: 0.0702 Acc: 0.9829\n",
      "Epoch 21/24\n",
      "----------\n",
      "train Loss: 0.0714 Acc: 0.9829\n",
      "Epoch 22/24\n",
      "----------\n",
      "train Loss: 0.0708 Acc: 0.9840\n",
      "Epoch 23/24\n",
      "----------\n",
      "train Loss: 0.0652 Acc: 0.9842\n",
      "Epoch 24/24\n",
      "----------\n",
      "train Loss: 0.0586 Acc: 0.9842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNetTrain(\"./archive/flowers\", \"\")\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test \n",
    "\n",
    "- input (3),  trained model, path to the image folder, path to the csv with image paths and [optional: classes]\n",
    "- output (2): 2D matrix of predictions (floats) for each image and class,  [optional, if ground truth is provided: accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetTest:\n",
    "    def __init__(self, trained_model, img_folder_path=\"./test\", img_csv=\"./test.csv\")\n",
    "        self.img_folder_path = img_folder_path\n",
    "        self.img_csv = img_csv\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cd1a84c6e748890eba993db88b6a1a54ba5ad1541d2f6351ed40b87eab1cc0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
